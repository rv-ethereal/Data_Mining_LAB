# **Retail Data lake  Pipeline**

*A complete end-to-end Data Engineering project using Apache Airflow, Apache Spark, MinIO Data Lake, Docker, and Apache Superset.*

---

## ğŸš€ **Project Overview**

This project demonstrates a real-world **Retail Analytics Data Engineering Pipeline**.
It covers everything from **raw data ingestion â†’ ETL processing â†’ data warehousing â†’ orchestrated workflows â†’ analytics dashboard**.

The architecture is built using:

* **Apache Airflow** for workflow orchestration
* **Apache Spark** for ETL processing
* **MinIO (S3 compatible)** as the Data Lake
* **Docker Compose** to run everything seamlessly
* **Apache Superset** for BI dashboards and reporting

This project showcases strong practical skills in ETL, data pipelines, scheduling, monitoring, and analytics.

##  **Architecture Diagram**

The pipeline follows a structured multi-zone Data Lake approach:

```
Raw â†’ ETL â†’ Warehouse â†’ Analytics â†’ Dashboard
```

Each layer has its own folder inside the `datalake/` directory.

* **raw/** â€” raw CSV and JSON files
* **warehouse/** â€” processed & cleaned Parquet files
* **analytics/** â€” ready-to-use analytics datasets

---

## ğŸ“‚ **Project Structure**

```
MSD24011-Data_Mining_LAB/
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ retail_etl_dag.py
â”‚
â”œâ”€â”€ spark/
â”‚   â””â”€â”€ spark_etl.py
â”‚
â”œâ”€â”€ datalake/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ customers.csv
â”‚   â”‚   â”œâ”€â”€ products.json
â”‚   â”‚   â””â”€â”€ sales.csv
â”‚   â”‚
â”‚   â”œâ”€â”€ warehouse/sales_fact/
â”‚   â”‚   â””â”€â”€ *.parquet
â”‚   â”‚
â”‚   â””â”€â”€ analytics/sales_fact/
â”‚       â””â”€â”€ *.parquet
â”‚
â”œâ”€â”€ superset/
â”‚   â””â”€â”€ superset_config.py
â”‚
â”œâ”€â”€ Result_images/
â”‚   â”œâ”€â”€ Airflow_Dag.jpg
â”‚   â”œâ”€â”€ Airflow_UI.jpg
â”‚   â”œâ”€â”€ dashboard.jpg
â”‚   â”œâ”€â”€ Spark_master_UI.jpg
â”‚   â”œâ”€â”€ Spark_worker_UI.jpg
â”‚   â””â”€â”€ More Screenshots...
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ dashboard.py
â”œâ”€â”€ .env
â””â”€â”€ README.md  (this one)
```

---

## ğŸ”„ **Pipeline Workflow**

### **1ï¸âƒ£ Raw Data Ingestion**

Raw files stored in:

```
/datalake/raw/customers.csv
/datalake/raw/products.json
/datalake/raw/sales.csv
```

These represent customer info, product master data, and sales transactions.

---

### **2ï¸âƒ£ ETL using Apache Spark**

The script:
`/spark/spark_etl.py`

Performs:

* Loading CSV & JSON files from Raw zone
* Data type cleaning, date parsing
* Joining customer, product & sales data
* Creating a **Sales Fact table**
* Saving output as Parquet into:

```
/datalake/warehouse/sales_fact
```

---

### **3ï¸âƒ£ Workflow Orchestration using Apache Airflow**

DAG file:
`airflow/dags/retail_etl_dag.py`

The DAG includes:

* Spark ETL trigger
* Validations
* Timestamped logs
* Automated daily scheduling

Images included:

* âœ” Airflow DAG Screenshot
* âœ” Airflow UI
* âœ” Task Graph view

---

### **4ï¸âƒ£ Analytics Layer**

Spark creates analytics-friendly datasets:

```
/datalake/analytics/sales_fact/
```

Used later for BI dashboards.

---

### **5ï¸âƒ£ BI Dashboard using Apache Superset**

Config file:
`/superset/superset_config.py`

Dashboard built on:

* Total Sales
* Top Customers
* Product Performance
* Monthly Trends
* Interactive Filters


---

## ğŸ³ **Docker Compose Setup**

The `docker-compose.yml` file creates:

* Airflow Scheduler & WebServer
* Spark Master & Workers
* MinIO S3 Bucket
* Superset Dashboard

To run the complete setup:

```bash
docker-compose up -d
```

---

## ğŸ“Š **Screenshots**

Screenshots included in `Result_images/` folder:

* Airflow DAG
* Airflow UI
* Spark Master UI
* Spark Worker UI
* Superset Dashboard
* Results Preview

