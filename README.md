
# <a id="readme-top"></a>

<div align="center">

<img src="https://cdn-icons-png.flaticon.com/512/3135/3135715.png" alt="Logo" width="120" height="120">

# ğŸš€ On-Premise Data Lake with Apache Spark ETL, Apache Airflow Orchestration & Apache Superset Dashboards

### **Data Mining Laboratory Project**

**Under the guidance of *Prof. Sandeep Kumar Srivastava***

</div>

---

## ğŸ“Š GitHub Repository Badges

[![Repo Stars](https://img.shields.io/github/stars/rv-ethereal/Data_Mining_LAB?style=for-the-badge)](https://github.com/rv-ethereal/Data_Mining_LAB/stargazers)
[![Repo Forks](https://img.shields.io/github/forks/rv-ethereal/Data_Mining_LAB?style=for-the-badge)](https://github.com/rv-ethereal/Data_Mining_LAB/network/members)
[![Repo Issues](https://img.shields.io/github/issues/rv-ethereal/Data_Mining_LAB?style=for-the-badge)](https://github.com/rv-ethereal/Data_Mining_LAB/issues)
[![Contributors](https://img.shields.io/github/contributors/rv-ethereal/Data_Mining_LAB?style=for-the-badge)](https://github.com/rv-ethereal/Data_Mining_LAB/graphs/contributors)
[![License](https://img.shields.io/github/license/rv-ethereal/Data_Mining_LAB?style=for-the-badge)](LICENSE)

---

<div align="center">

## ğŸ¬ Dashboard Demo (GIF Preview)

<img src="https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExNmFvaTEyM3h1czFoN25ja2U2N2l0d3gzNWMzZjMzYmx3aGg2Y3QwaiZlcD12MV9naWZzX3NlYXJjaCZjdD1n/3oEjI6SIIHBdRxXI40/giphy.gif" width="700"/>

> *Replace this GIF with your own Superset dashboard screen recording later.*

</div>

---

## ğŸ“Œ About the Project

This is a **complete on-premise data engineering pipeline**, built as part of the **Data Mining Laboratory** under **Prof. Sandeep Kumar Srivastava**.

The project simulates a **real-world enterprise data system**, implemented entirely on a **local machine** (no cloud):

* A structured **local data lake**
* **Apache Spark** for ETL processing
* **Apache Airflow** for scheduling & orchestration
* **Apache Superset** for dashboards

This framework provides a full workflow for **data ingestion â†’ transformation â†’ warehousing â†’ visualization**.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

---

## ğŸ— System Architecture Overview

```
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚     Data Sources      â”‚
                      â”‚  CSV / JSON / APIs    â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚     On-Premise Data Lake     â”‚
                  â”‚  raw / staging / processed   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚         Apache Airflow (Scheduler)        â”‚
          â”‚  Triggers Spark ETL on schedule           â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚           Apache Spark ETL Pipeline       â”‚
          â”‚   Cleaning | Transforming | Aggregations  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚    Local Data Warehouse (Parquet / SQLite / PostgreSQL)   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚         Apache Superset           â”‚
     â”‚     Dashboards & Visual Analytics â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ Data Lake Structure

```
datalake/
    â”œâ”€â”€ raw/
    â”‚     â”œâ”€â”€ sales.csv
    â”‚     â”œâ”€â”€ products.json
    â”‚     â”œâ”€â”€ customers.csv
    â”œâ”€â”€ staging/
    â”œâ”€â”€ processed/
    â””â”€â”€ warehouse/
```

---

## ğŸ§° Built With

### **Storage**

* Local File System (Data Lake)

### **Processing**

* Apache Spark
* PySpark

### **Workflow Orchestration**

* Apache Airflow (DAG-based automation)

### **Analytics**

* Apache Superset

### **Warehouse Engine**

* Parquet
* SQLite / PostgreSQL (Optional)

<p align="right">(<a href="#readme-top">back to top</a>)</p>

---

## ğŸ”§ Detailed Workflow

### **1ï¸âƒ£ Data Ingestion â€” Raw Zone**

Sources include:

* CSV files
* JSON dumps
* API exports (optional)

All files are placed inside:

```
datalake/raw/
```

---

### **2ï¸âƒ£ Apache Airflow DAG â€” Pipeline Automation**

Airflow manages:

âœ” Ingest raw files
âœ” Trigger Spark ETL job
âœ” Validate processed outputs
âœ” Load curated datasets into warehouse
âœ” Trigger Superset refresh (optional)

Example DAG:

```python
with DAG('spark_etl_pipeline', schedule_interval='@daily') as dag:

    ingest = BashOperator(...)
    spark_etl = SparkSubmitOperator(...)
    validate = PythonOperator(...)
    load_warehouse = BashOperator(...)
```

---

### **3ï¸âƒ£ Apache Spark ETL â€” Transform Phase**

Spark performs:

* Null and anomaly removal
* Data type conversions
* Feature engineering
* Aggregations
* Joins between datasets
* Writing cleaned data to `/processed`
* Writing final analytics-ready data to `/warehouse`

Example:

```python
df = spark.read.csv("datalake/raw/sales.csv", header=True)
cleaned = df.dropna().withColumn("total", df.qty * df.price)
cleaned.write.mode("overwrite").parquet("datalake/processed/sales")
```

---

### **4ï¸âƒ£ Data Warehouse â€” Load Phase**

Data stored as:

* Parquet files
* OR SQL tables (SQLite/PostgreSQL)

---

### **5ï¸âƒ£ Apache Superset â€” Dashboard Layer**

Superset builds visualizations such as:

ğŸ“Š Revenue by Month
ğŸ“ˆ Sales Trend Analysis
ğŸ—º Region-wise Sales Map
ğŸ“¦ Top Products by Revenue
ğŸ‘¤ Customers by Location
ğŸ“‰ Return Percentage

All panels combined into a clean BI dashboard.

<p align="right">(<a href="#readme-top">back to top</a>)</p>

---

## ğŸ¯ Final Deliverables

âœ” Complete On-Prem Data Lake
âœ” Spark ETL PySpark Scripts
âœ” Airflow Automated DAG
âœ” Structured Warehouse (Parquet/SQL)
âœ” Superset Dashboard (6â€“10 charts)
âœ” Architecture Diagram
âœ” Detailed Project Documentation + README

---

## ğŸ“ Contact

**Student:** (Add your name here)
**Instructor:** *Prof. Sandeep Kumar Srivastava*
**Repository:** [https://github.com/rv-ethereal/Data_Mining_LAB](https://github.com/rv-ethereal/Data_Mining_LAB)

<p align="right">(<a href="#readme-top">back to top</a>)</p>


