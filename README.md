
---

# On-Premises Data Lake using Apache Spark, Airflow & Superset

A complete **on-premises data lake and analytics pipeline** built using **Apache Spark for ETL processing**, **Apache Airflow for workflow orchestration**, and **Apache Superset for business intelligence and visualization**.
This project demonstrates the full **Raw â†’ Processed â†’ Warehouse â†’ Analytics** lifecycle for structured enterprise data.

---

## ğŸ¯ Project Objective

To design and implement a **local data lake architecture** that:

* Ingests raw transactional data
* Performs scalable ETL using Apache Spark
* Automates workflows using Apache Airflow
* Builds analytical warehouse tables
* Visualizes business insights using Apache Superset

This project is developed as part of the **Data Mining and Warehousing Laboratory**.

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Raw Data Layer                       â”‚
â”‚        (sales.csv, customers.csv)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Apache Airflow Orchestration                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ File Check  â”‚â”€â–¶â”‚Spark ETL â”‚â”€â–¶â”‚ Output Validate â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Processed Data Layer                         â”‚
â”‚  sales_clean | customers_clean | sales_with_customers  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Warehouse Analytics Layer                   â”‚
â”‚  revenue_by_product | revenue_by_region                 â”‚
â”‚  payment_analysis | status_summary                      â”‚
â”‚  customer_summary | monthly_sales                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Apache Superset (Analytics Dashboard)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Technology Stack

* **Apache Spark 3.5+** â€“ Distributed ETL processing
* **Apache Airflow 2.6.3** â€“ Workflow orchestration
* **Apache Superset** â€“ Business Intelligence dashboard
* **Python 3.11+** â€“ Core programming language
* **Pandas & PyArrow** â€“ Data handling
* **SQLite** â€“ Analytical warehouse database

---

## ğŸ“ Project Structure

```
onprem-datalake/
â”œâ”€â”€ airflow/dags/
â”‚   â””â”€â”€ spark_etl_dag.py        # Daily Airflow ETL pipeline
â”œâ”€â”€ spark/
â”‚   â””â”€â”€ spark_etl.py            # Spark ETL with Pandas fallback
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ parquet_to_sqlite.py    # Warehouse Parquet â†’ SQLite converter
â”œâ”€â”€ datalake/
â”‚   â”œâ”€â”€ raw/                    # Input CSV files
â”‚   â”œâ”€â”€ processed/              # Cleaned Parquet data
â”‚   â””â”€â”€ warehouse/              # Aggregated analytics tables
â”œâ”€â”€ app.py                      # Superset application entry
â”œâ”€â”€ superset_config.py          # Superset configuration
â”œâ”€â”€ init_superset.ps1           # Superset initialization script
â”œâ”€â”€ run_superset.ps1            # Script to start Superset
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”„ ETL Pipeline Description

**Airflow DAG:** `data_lake_etl_pipeline`
**Schedule:** Daily

### Pipeline Tasks:

1. **Check Input Files** â€“ Verifies presence of raw CSV files
2. **Run Spark ETL** â€“ Data cleaning, transformation, and aggregation
3. **Validate Warehouse Output** â€“ Confirms all analytics tables are created
4. **Success Notification**

The Spark ETL also includes a **Pandas fallback mode** to ensure execution even if Spark fails.

---

## ğŸ“Š Warehouse Analytics Tables

| Table Name           | Description                    |
| -------------------- | ------------------------------ |
| `revenue_by_product` | Product-wise sales and revenue |
| `revenue_by_region`  | Regional business performance  |
| `payment_analysis`   | Payment method statistics      |
| `status_summary`     | Order status distribution      |
| `customer_summary`   | Customer purchase behavior     |
| `monthly_sales`      | Monthly business trend         |

---

## âš™ï¸ Setup Instructions (Windows)

### 1ï¸âƒ£ Install Dependencies

```powershell
pip install -r requirements.txt
```

---

### 2ï¸âƒ£ Prepare Input Data

Place the following files in:

```
datalake/raw/
- sales.csv
- customers.csv
```

---

### 3ï¸âƒ£ Initialize Superset

```powershell
.\init_superset.ps1
```

This will:

* Initialize Superset database
* Create default admin user
* Set up roles and permissions

---

### 4ï¸âƒ£ Start Services

**Option 1: Airflow Standalone Mode**

```powershell
airflow standalone
```

**Option 2: Manual Services**

```powershell
airflow webserver --port 8080
airflow scheduler
```

**Start Superset**

```powershell
.\run_superset.ps1
```

* Airflow UI â†’ [http://localhost:8080](http://localhost:8080)
* Superset UI â†’ [http://localhost:8088](http://localhost:8088)
* Superset Login â†’ `admin / admin`

---

## â–¶ï¸ Trigger ETL Pipeline

```powershell
airflow dags trigger data_lake_etl_pipeline
```

You may also trigger the DAG directly from the **Airflow Web UI**.

---

## ğŸ§ª Run Spark ETL Manually

```powershell
spark-submit spark/spark_etl.py
```

With custom memory:

```powershell
spark-submit --driver-memory 4g --executor-memory 4g spark/spark_etl.py
```

---

## ğŸ—„ï¸ Convert Warehouse Parquet to SQLite (For Superset)

```powershell
python tools/parquet_to_sqlite.py
```

This generates:

```
datalake/warehouse.db
```

which is used as the **Superset data source**.

---

## â­ Key Features

* Fully automated **daily ETL pipeline**
* **Spark + Pandas fallback system**
* Multi-layer **On-Premise Data Lake architecture**
* Workflow monitoring with **Apache Airflow**
* Interactive dashboards with **Apache Superset**
* Lightweight **SQLite warehouse integration**



---

## ğŸ“š References

* Apache Spark Documentation
* Apache Airflow Documentation
* Apache Superset Documentation

---

## ğŸ‘¨â€ğŸ“ Author

* Gaurav Kumar (MSA24002)
* Course: Data Mining and Warehousing Laboratory
* Project Type:** On-Premises Data Lake Implementation

---
