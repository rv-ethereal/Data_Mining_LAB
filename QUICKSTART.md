# ğŸ‰ Data Lakehouse - System Ready!

## âœ… All Systems Operational

### Services Status
| Service | Status | URL | Credentials |
|---------|--------|-----|-------------|
| **Airflow** | âœ… Running | http://localhost:8082 | admin/admin |
| **Superset** | âœ… Running | http://localhost:8088 | admin/admin |
| **Spark Master** | âœ… Running | http://localhost:8080 | - |
| **Spark Worker** | âœ… Running | http://localhost:8081 | - |
| **MinIO** | âœ… Healthy | http://localhost:9001 | admin/minio123 |
| **Trino** | âœ… Healthy | http://localhost:8083 | - |
| **PostgreSQL** | âœ… Healthy | Internal | - |

### Recent Fixes
- âœ… **Superset Login** - Admin user created
- âœ… **Docker API** - Airflow containers upgraded to Docker CLI v26.1.3
- âœ… **Sample Data** - 4 CSV files (7.2 MB) uploaded to MinIO

### Data Uploaded
- âœ… customers.csv (652 KB)
- âœ… products.csv (31 KB)  
- âœ… orders.csv (1.6 MB)
- âœ… order_items.csv (4.9 MB)

## ğŸš€ Next Steps

### Option 1: Use Airflow UI (Recommended)

1. **Open Airflow**: http://localhost:8082
2. **Login**: admin/admin
3. **Find DAG**: Look for `lakehouse_pipeline`
4. **Enable**: Toggle the switch to enable the DAG
5. **Trigger**: Click the "â–¶" Play button to run
6. **Monitor**: Watch in Grid or Graph view

### Option 2: Command Line

```bash
# Trigger the pipeline
docker exec lakehouse-airflow-webserver airflow dags trigger lakehouse_pipeline

# Monitor status
docker exec lakehouse-airflow-webserver airflow dags list-runs -d lakehouse_pipeline
```

### Option 3: Manual Execution (For Testing)

If you want to run Spark jobs directly without Airflow:

#### Bronze Layer
```bash
docker exec lakehouse-spark-master /opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --jars /opt/spark/extra-jars/delta-core_2.12-2.4.0.jar,/opt/spark/extra-jars/delta-storage-2.4.0.jar,/opt/spark/extra-jars/hadoop-aws-3.3.4.jar,/opt/spark/extra-jars/aws-java-sdk-bundle-1.12.262.jar \
  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \
  --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \
  /opt/spark-apps/jobs/bronze_ingestion.py
```

#### Silver Layer
```bash
docker exec lakehouse-spark-master /opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --jars /opt/spark/extra-jars/delta-core_2.12-2.4.0.jar,/opt/spark/extra-jars/delta-storage-2.4.0.jar,/opt/spark/extra-jars/hadoop-aws-3.3.4.jar,/opt/spark/extra-jars/aws-java-sdk-bundle-1.12.262.jar \
  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \
  --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \
  /opt/spark-apps/jobs/silver_transformation.py
```

#### Gold Layer
```bash
docker exec lakehouse-spark-master /opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --jars /opt/spark/extra-jars/delta-core_2.12-2.4.0.jar,/opt/spark/extra-jars/delta-storage-2.4.0.jar,/opt/spark/extra-jars/hadoop-aws-3.3.4.jar,/opt/spark/extra-jars/aws-java-sdk-bundle-1.12.262.jar \
  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \
  --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \
  /opt/spark-apps/jobs/gold_aggregation.py
```

## ğŸ“Š After Pipeline Runs

### Query Data in Trino

```bash
# Connect to Trino CLI
docker exec -it lakehouse-trino trino

# Run queries
SHOW SCHEMAS FROM delta;
USE delta.gold;
SHOW TABLES;
SELECT * FROM daily_sales LIMIT 10;
```

### Create Superset Dashboard

1. **Login to Superset**: http://localhost:8088 (admin/admin)

2. **Add Database Connection**:
   - Click **Settings** â†’ **Database Connections**
   - Click **+ Database**
   - Select **Trino** from the list
   - **SQLAlchemy URI**: `trino://admin@trino:8080/delta`
   - Click **Test Connection**
   - Click **Connect**

3. **Create Dataset**:
   - Go to **Data** â†’ **Datasets**
   - Click **+ Dataset**
   - **Database**: Select your Trino connection
   - **Schema**: `gold`
   - **Table**: `daily_sales` (or other tables)
   - Click **Add**

4. **Create Charts**:
   - Go to **Charts** â†’ **+ Chart**
   - Select your dataset
   - Choose visualization type (Bar Chart, Line Chart, etc.)
   - Configure metrics and filters
   - Save to dashboard

## ğŸ” Monitoring & Debugging

### Check Airflow Logs
```bash
docker logs lakehouse-airflow-scheduler --tail 50
docker logs lakehouse-airflow-webserver --tail 50
```

### Check Spark Logs
```bash
docker logs lakehouse-spark-master --tail 50
docker logs lakehouse-spark-worker --tail 50
```

### Check MinIO Buckets
```bash
docker exec lakehouse-minio mc ls minio/bronze/
docker exec lakehouse-minio mc ls minio/silver/
docker exec lakehouse-minio mc ls minio/gold/
```

### Verify Trino Connection
```bash
curl http://localhost:8083/v1/info
```

## ğŸ“ Project Structure

```
MSD24001-L1/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ lakehouse_pipeline.py  # Main ETL DAG
â”‚   â”œâ”€â”€ logs/                       # Airflow logs
â”‚   â”œâ”€â”€ plugins/                    # Custom plugins
â”‚   â””â”€â”€ Dockerfile                  # Custom Airflow with updated Docker CLI
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â”œâ”€â”€ bronze_ingestion.py      # Raw data ingestion
â”‚   â”‚   â”œâ”€â”€ silver_transformation.py # Data quality & cleaning
â”‚   â”‚   â””â”€â”€ gold_aggregation.py      # Business metrics
â”‚   â””â”€â”€ jars/                        # Delta Lake JARs
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample/
â”‚       â”œâ”€â”€ generator.py             # Sample data generator
â”‚       â”œâ”€â”€ customers.csv
â”‚       â”œâ”€â”€ products.csv
â”‚       â”œâ”€â”€ orders.csv
â”‚       â””â”€â”€ order_items.csv
â”œâ”€â”€ trino/
â”‚   â””â”€â”€ catalog/
â”‚       â””â”€â”€ delta.properties         # Delta Lake connector config
â”œâ”€â”€ superset/
â”‚   â””â”€â”€ Dockerfile                   # Custom Superset with Trino driver
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ upload_to_minio.py          # Data upload utility
â”œâ”€â”€ docker-compose.yml               # Container orchestration
â”œâ”€â”€ .env                            # Environment variables
â”œâ”€â”€ README.md                       # Full documentation
â””â”€â”€ QUICKSTART.md                   # This file
```

## ğŸ¯ Expected Pipeline Results

After running the full pipeline, you will have:

### Bronze Layer
- Raw CSV data ingested from MinIO
- Stored in Delta Lake format
- Full data lineage preserved

### Silver Layer
- Cleaned and validated data
- Data quality checks applied
- Standardized schemas

### Gold Layer (Business Metrics)
- `daily_sales` - Daily revenue & transactions
- `product_performance` - Product sales analytics
- `customer_analytics` - Customer behavior metrics

## ğŸ› ï¸ Troubleshooting

### Airflow DAG Not Appearing
```bash
# Refresh DAGs
docker exec lakehouse-airflow-webserver airflow dags list

# Check for import errors
docker exec lakehouse-airflow-webserver airflow dags list-import-errors
```

### Spark Job Fails
```bash
# Check if JARs exist
docker exec lakehouse-spark-master ls -la /opt/spark/extra-jars/

# Test manual submission
docker exec lakehouse-spark-master /opt/spark/bin/spark-submit --version
```

### Trino Can't Query Delta Tables
```bash
# Check Trino catalog
docker logs lakehouse-trino | grep delta

# Verify metastore location in MinIO
docker exec lakehouse-minio mc ls minio/gold/metastore/
```

### Superset Connection Error
1. Verify Trino is running: `curl http://localhost:8083/v1/info`
2. Check URI format: `trino://admin@trino:8080/delta`
3. Ensure no typos in database name (`delta` not `Delta`)

## ğŸ“š Additional Resources

- **Airflow Docs**: https://airflow.apache.org/docs/
- **Spark Docs**: https://spark.apache.org/docs/latest/
- **Delta Lake**: https://docs.delta.io/
- **Trino**: https://trino.io/docs/current/
- **Superset**: https://superset.apache.org/docs/intro

---

## ğŸ‰ You're All Set!

Your data lakehouse is now fully operational with:
- âœ… Automated ETL orchestration via Airflow
- âœ… Distributed processing with Spark
- âœ… ACID transactions with Delta Lake
- âœ… Fast SQL queries with Trino
- âœ… Interactive dashboards with Superset

Start by triggering the pipeline in Airflow or run the manual commands above!

**Happy Data Engineering! ğŸš€**
