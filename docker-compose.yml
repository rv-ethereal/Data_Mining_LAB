version: '3.8'

services:
  # MinIO - S3 compatible object storage
  minio:
    image: minio/minio:latest
    container_name: lakehouse-minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    networks:
      - lakehouse-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  # MinIO bucket initialization
  minio-init:
    image: minio/mc:latest
    container_name: lakehouse-minio-init
    depends_on:
      - minio
    entrypoint: >
      /bin/sh -c "
      sleep 10;
      /usr/bin/mc alias set myminio http://minio:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD};
      /usr/bin/mc mb myminio/bronze --ignore-existing;
      /usr/bin/mc mb myminio/silver --ignore-existing;
      /usr/bin/mc mb myminio/gold --ignore-existing;
      /usr/bin/mc mb myminio/raw-data --ignore-existing;
      echo 'Buckets created successfully';
      exit 0;
      "
    networks:
      - lakehouse-network

  # Spark Master
  spark-master:
    image: apache/spark-py:v3.4.0
    container_name: lakehouse-spark-master
    user: root
    environment:
      - SPARK_MODE=master
      - SPARK_NO_DAEMONIZE=true
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - ./spark:/opt/spark-apps
      - ./data:/opt/spark-data
      - ./spark/jars:/opt/spark/extra-jars:ro
      - spark-work:/opt/spark/work
    networks:
      - lakehouse-network
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master

  # Spark Worker
  spark-worker:
    image: apache/spark-py:v3.4.0
    container_name: lakehouse-spark-worker
    user: root
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_MEMORY=4G
      - SPARK_WORKER_CORES=4
      - SPARK_NO_DAEMONIZE=true
    ports:
      - "8081:8081"
    volumes:
      - ./spark:/opt/spark-apps
      - ./data:/opt/spark-data
      - ./spark/jars:/opt/spark/extra-jars:ro
      - spark-work:/opt/spark/work
    networks:
      - lakehouse-network
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077 --webui-port 8081

  # PostgreSQL - Airflow metadata database
  postgres:
    image: postgres:14
    container_name: lakehouse-postgres
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - lakehouse-network
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Airflow Init - Initialize database and create admin user
  airflow-init:
    image: apache/airflow:2.8.1-python3.11
    container_name: lakehouse-airflow-init
    depends_on:
      - postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      - AIRFLOW__CORE__FERNET_KEY=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - _AIRFLOW_DB_MIGRATE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=${AIRFLOW_ADMIN_USERNAME}
      - _AIRFLOW_WWW_USER_PASSWORD=${AIRFLOW_ADMIN_PASSWORD}
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db migrate
        airflow users create \
          --username ${AIRFLOW_ADMIN_USERNAME} \
          --password ${AIRFLOW_ADMIN_PASSWORD} \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email ${AIRFLOW_ADMIN_EMAIL} || true
    networks:
      - lakehouse-network

  # Airflow Webserver
  airflow-webserver:
    image: apache/airflow:2.8.1-python3.11
    container_name: lakehouse-airflow-webserver
    depends_on:
      - postgres
      - airflow-init
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      - AIRFLOW__CORE__FERNET_KEY=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session
      - AIRFLOW__WEBSERVER__SECRET_KEY=mysecretkey
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./spark:/opt/spark-apps
      - ./data:/opt/spark-data
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "8082:8080"
    command: webserver
    networks:
      - lakehouse-network

  # Airflow Scheduler
  airflow-scheduler:
    image: apache/airflow:2.8.1-python3.11
    container_name: lakehouse-airflow-scheduler
    depends_on:
      - postgres
      - airflow-init
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      - AIRFLOW__CORE__FERNET_KEY=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./spark:/opt/spark-apps
      - ./data:/opt/spark-data
      - /var/run/docker.sock:/var/run/docker.sock
    command: scheduler
    networks:
      - lakehouse-network

  # Trino - Distributed SQL query engine
  trino:
    image: trinodb/trino:latest
    container_name: lakehouse-trino
    ports:
      - "8083:8080"
    volumes:
      - ./trino/catalog:/etc/trino/catalog
    networks:
      - lakehouse-network

  # Apache Superset - Data visualization
  superset:
    build: ./superset
    container_name: lakehouse-superset
    environment:
      - SUPERSET_SECRET_KEY=${SUPERSET_SECRET_KEY}
    ports:
      - "8088:8088"
    volumes:
      - superset-data:/app/superset_home
    networks:
      - lakehouse-network
    command: >
      /bin/sh -c "
      superset db upgrade &&
      superset fab create-admin \
        --username ${SUPERSET_ADMIN_USERNAME} \
        --firstname Admin \
        --lastname User \
        --email ${SUPERSET_ADMIN_EMAIL} \
        --password ${SUPERSET_ADMIN_PASSWORD} || true &&
      superset init &&
      superset run -h 0.0.0.0 -p 8088 --with-threads --reload
      "

volumes:
  minio-data:
  postgres-data:
  superset-data:
  spark-work:

networks:
  lakehouse-network:
    driver: bridge
